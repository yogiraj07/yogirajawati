{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Author : Yogiraj Awati\n",
    "from pyspark.ml.feature import PCA,Tokenizer,RegexTokenizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\",\"simple app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CONFERENCE_PREFIX=\"#*\"\n",
    "publicationPath = \"/home/yogi/Desktop/hw3/publications.txt\"\n",
    "stopWordsPath=\"/home/yogi/Desktop/hw3/stopwords_english.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Read data\n",
    "destinationFile = sc.textFile(publicationPath)\n",
    "#get titles\n",
    "titlesRdd = destinationFile.filter(lambda x: x!=\"\" and x.startswith(CONFERENCE_PREFIX)).map(lambda line : line[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Tokenize words\n",
    "from pyspark.sql import Row\n",
    "row = Row(\"val\")\n",
    "publicationTitles = titlesRdd.map(row).toDF()\n",
    "tokenizer = RegexTokenizer(inputCol=\"val\",outputCol=\"words\",pattern=\"\\\\W\")\n",
    "tokenizedWords = tokenizer.transform(publicationTitles) \n",
    "# we get list of rows where each row is a sentence and list of words in that sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Count words and create sparse vectors\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "TOP_FREQUENT_WORDS = 1000\n",
    "\n",
    "def perform_countvector_operation(tokenizedWords,inputColName):\n",
    "    cv = CountVectorizer(inputCol=inputColName, outputCol=\"features\", vocabSize=TOP_FREQUENT_WORDS)\n",
    "    # fit a CountVectorizerModel from the corpus.\n",
    "    cv_model = cv.fit(tokenizedWords)\n",
    "    count_vectors = cv_model.transform(tokenizedWords)\n",
    "    print (\"Count Vectors: \")\n",
    "    count_vectors.show(truncate=True)\n",
    "    return cv_model,count_vectors\n",
    "\n",
    "def perfom_pca(count_vectors,NUMBER_OF_COMPONENTS):\n",
    "    pca = PCA(k=NUMBER_OF_COMPONENTS, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "    pca_model = pca.fit(count_vectors)\n",
    "    pca_result = pca_model.transform(count_vectors).select(\"pcaFeatures\")\n",
    "    pca_result.show(truncate=True)\n",
    "    return pca_model,pca_result\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def render_graph(pca_model):\n",
    "    eigen_values = pca_model.explainedVariance\n",
    "    plt.plot(eigen_values)\n",
    "    plt.ylabel(\"Eigen Value\")\n",
    "    plt.xlabel(\"Principal Component\")\n",
    "    print (\"Drawing plot:\")\n",
    "    plt.show()\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "def calculateKofTotalVariance(pca_model,k):\n",
    "    eigen_values = pca_model.explainedVariance\n",
    "    totalSumOfEigenValues = np.sum(eigen_values) # get total sum of the eigen_values\n",
    "    requiredSum = (k * totalSumOfEigenValues)/100 # get 50 % of the total sum\n",
    "    numberofComponents = 0\n",
    "    cumsum = 0\n",
    "    for v in eigen_values:\n",
    "        cumsum = cumsum + v\n",
    "        numberofComponents = numberofComponents + 1\n",
    "        if cumsum >= requiredSum:\n",
    "            print (\"Number of components required for 50% of the total variance: \",numberofComponents)\n",
    "            break\n",
    "def calculateImportantWords(cv_model,pca_model):\n",
    "    vocab = cv_model.vocabulary\n",
    "    pc_array = pca_model.pc.toArray()\n",
    "    transpose_array = pc_array.transpose()\n",
    "    words = set() #stores unique words from whose value is > 0.2\n",
    "    count = 0\n",
    "    loop = True\n",
    "    while (loop):\n",
    "        for index,value in enumerate (transpose_array[count]): # for each row in transpose matrix\n",
    "            if (np.abs(value) > 0.2) :\n",
    "                # fetch corresponsing word from the vocab\n",
    "                words.add(vocab[index])\n",
    "        count = count + 1\n",
    "        if(count == len(transpose_array) - 1): #until last index of the array\n",
    "              loop = False\n",
    "    print (\"Important Unique Words: \",len(words))\n",
    "    print (words)\n",
    "\n",
    "def render_scatter_plot(pca_result):\n",
    "    sampleData = pca_result.take(1000)\n",
    "    scatterInput=[]\n",
    "    for s in sampleData:\n",
    "        scatterInput.append(s[0]) #convert to array\n",
    "    transpose_input= np.array(scatterInput).transpose() #take transpose\n",
    "\n",
    "    xAxis=transpose_input[0]\n",
    "    yAxis=transpose_input[1]\n",
    "\n",
    "    plt.scatter(xAxis,yAxis)\n",
    "    print(\"Scatter plot: \")\n",
    "    plt.show()\n",
    "\n",
    "def performOperation(tokenizedWords,inputColName):\n",
    "    s_cv_model,s_count_vectors= perform_countvector_operation(tokenizedWords,inputColName)\n",
    "    s_pca_model,s_pca_result = perfom_pca(s_count_vectors,50) #do pca analysis\n",
    "    render_graph(s_pca_model)\n",
    "    calculateKofTotalVariance(s_pca_model,50)\n",
    "    calculateImportantWords(cv_model,pca_model)\n",
    "    render_scatter_plot(s_pca_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2A) Transform titles to word count vectors. Truncate your (sparse) vectors to the 1000 most frequent words and perform PCA with 50 components on the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cv_model,count_vectors= perform_countvector_operation(tokenizedWords,\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Do PCA Analysis\n",
    "#PCA for 50 components\n",
    "pca_model,pca_result = perfom_pca(count_vectors,50) #do pca analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2B)\n",
    "Plot the eigenvalues of the principal components. Calculate how many components\n",
    "are needed to explain 50% of the total variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "render_graph(pca_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "calculateKofTotalVariance(pca_model,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2C)Identify which words are important in each of the principal components. To do so, take the sum of squares of each of the component vectors to check how they are normalized. For each component, then print out the words for which the\n",
    "absolute value of the component is larger than 0.20 of the norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculateImportantWords(cv_model,pca_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2D) Make a scatter plot of some reasonably sized sample (1k-10k titles). \n",
    "Explain the structure (or lack thereof) you see based on your results from item b-c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "render_scatter_plot(pca_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2E) Run a preprocessing step to remove stop words (a list of stop words is provided which is identical list used in Spark). Rerun steps b-d and evaluate whether this has improved your representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stopWordsPointer = sc.textFile(stopWordsPath)\n",
    "stopWordsList = stopWordsPointer.collect()\n",
    "stopWordsRemover = StopWordsRemover(inputCol = \"words\",outputCol = \"reducedWords\",stopWords=stopWordsList)\n",
    "\n",
    "validWords = stopWordsRemover.transform(tokenizedWords)\n",
    "\n",
    "s_cv_model,s_count_vectors= perform_countvector_operation(validWords,\"reducedWords\")\n",
    "s_pca_model,s_pca_result = perfom_pca(s_count_vectors,50) #do pca analysis\n",
    "render_graph(s_pca_model)\n",
    "calculateKofTotalVariance(s_pca_model,50)\n",
    "calculateImportantWords(s_cv_model,s_pca_model)\n",
    "render_scatter_plot(s_pca_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2F) Calculate TF-IDF features for all titles and rerun the operations in parts b-d of this exercise. How have your results changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF,IDF\n",
    "def performTF_IDF(dataFrame):\n",
    "    idf=IDF()\n",
    "    model=idf.fit(dataFrame)\n",
    "    result = model.transform(dataFrame)\n",
    "    return result\n",
    "\n",
    "tfIdfResult = performTF_IDF(s_count_vectors)\n",
    "performOperation(tfIdfResult,\"idfFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "construct two lists of titles: For venues NIPS and VLDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2I) Merge the two sets of titles. Construct both word count vectors and TF-IDF features. Repeat steps b-d and compare word count results to TF-IDF results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NIPS_title_set = set()\n",
    "VLDB_title_set = set()\n",
    "with open(publicationPath, \"r\") as fileReader:\n",
    "    PUBLICATION_VENUE_PREFIX = \"#c\"\n",
    "    TITLE_PREFIX=\"#*\"\n",
    "   \n",
    "    for eachLine in fileReader:  \n",
    "        if eachLine.startswith(TITLE_PREFIX):\n",
    "            readingPublication = True\n",
    "            title = eachLine[2:].strip()         \n",
    "        elif eachLine.startswith(PUBLICATION_VENUE_PREFIX) and readingPublication == True:\n",
    "            venue = eachLine[2:].strip()\n",
    "            if(venue in \"NIPS\" and title != \"\"):\n",
    "                NIPS_title_set.add(title)\n",
    "            if(venue in \"VLDB\" and title != \"\"):\n",
    "                VLDB_title_set.add(title)\n",
    "            title=\"\"      \n",
    "        elif eachLine == \"\\n\":\n",
    "            readingPublication = False\n",
    "            title=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(len (NIPS_title_set))\n",
    "print(len (VLDB_title_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titles = NIPS_title_set.union(VLDB_title_set)\n",
    "publicationTitles = sc.parallelize(titles).map(row).toDF()\n",
    "tokenizer = RegexTokenizer(inputCol=\"val\",outputCol=\"words\",pattern=\"\\\\W\")\n",
    "tokenizedWords = tokenizer.transform(publicationTitles) \n",
    "performOperation(tokenizedWords,\"words\") #performs all the operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print (len (titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "validWords = stopWordsRemover.transform(tokenizedWords)\n",
    "s_cv_model,s_count_vectors= perform_countvector_operation(validWords,\"reducedWords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tfIdfResult = performTF_IDF(s_count_vectors)\n",
    "performOperation(tfIdfResult,\"idfFeatures\") #performs all the operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2J) Now make a scatter plot of these two principal components, showing the titles from each subset in different colors. Again compare word counts and TF-IDF.Did PCA succeed in uncovering the differences between the communities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s_cv_model,count_vectors= perform_countvector_operation(tokenizedWords,\"words\")\n",
    "\n",
    "nips = []\n",
    "vldb = []\n",
    "pca = PCA(k=50, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "pca_model = pca.fit(count_vectors)\n",
    "pca_result = pca_model.transform(count_vectors)\n",
    "allTitles = pca_result.collect()\n",
    "\n",
    "for title in allTitles:\n",
    "    if title[0] in NIPS_title_set:\n",
    "        nips.append(title[3])\n",
    "    if title[0] in VLDB_title_set:\n",
    "        vldb.append(title[3])\n",
    "\n",
    "nipsArray = np.array(nips)\n",
    "nipsTranspose = nipsArray.transpose()\n",
    "x = nipsTranspose[0]\n",
    "y = nipsTranspose[1]\n",
    "plt.scatter(x, y, color='red')\n",
    "\n",
    "vldbArray = np.array(vldb)\n",
    "vldbTranspose = vldbArray.transpose()\n",
    "x = vldbTranspose[0]\n",
    "y = vldbTranspose[1]\n",
    "plt.scatter(x, y, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s_cv_model,s_count_vectors= perform_countvector_operation(validWords,\"reducedWords\")\n",
    "nips = []\n",
    "vldb = []\n",
    "pca = PCA(k=50, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "pca_model = pca.fit(count_vectors)\n",
    "pca_result = pca_model.transform(count_vectors)\n",
    "allTitles = pca_result.collect()\n",
    "\n",
    "for title in allTitles:\n",
    "    if title[0] in NIPS_title_set:\n",
    "        nips.append(title[3])\n",
    "    if title[0] in VLDB_title_set:\n",
    "        vldb.append(title[3])\n",
    "\n",
    "nipsArray = np.array(nips)\n",
    "nipsTranspose = nipsArray.transpose()\n",
    "x = nipsTranspose[0]\n",
    "y = nipsTranspose[1]\n",
    "plt.scatter(x, y, color='red')\n",
    "\n",
    "vldbArray = np.array(vldb)\n",
    "vldbTranspose = vldbArray.transpose()\n",
    "x = vldbTranspose[0]\n",
    "y = vldbTranspose[1]\n",
    "plt.scatter(x, y, color='blue')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
